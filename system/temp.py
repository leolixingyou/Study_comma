good morning and welcome to mobilize 2020 CES press conference before we get

started I just want to read a quick risk factor statement and we'll get this show started today's presentation contains

forward-looking statements all statements made that are not historical facts are subject to a number of risks and uncertainties and actual results may

differ materially please refer to intel's most recent earnings release Form 10-q and 10k filing available for

more information on the risk factors that could cause actual results to differ with that it's my pleasure to

introduce professor Amnon Joshua senior vice president Intel and president and CEO of mobile I welcome everyone to our

annual the press conference you know at Mobile I we're dealing with the entire spectrum from driving assist until fully

autonomous driving so it's a complex domain and every year it's getting more and more complex so I'll try to simplify

things and point out really on the major pillars the major challenges and then

how we at Mobile I and Intel are handling those challenges so I'll first

start with numbers of 2010 19 all crammed into a into one slide we shipped

seventeen point four million chips IQ chips this year over all above fifty

four million IQ chips since 2007 so this

is it's really an amazing number so 54 million cars with our with our technology we have a 46% cargo here on

here today we have ongoing 47 production

programs a production program it's a length of about two to three years once we get a design win until the

product is on the road so we have 47 design wins ongoing these are production a program that are

ongoing we had 16 product launches so a product launch is in you

it's a new car model with a new bundle normally it's with an upgraded IQ chip

so we had 16 product launches and 33 design wins this year 2019 so this is

2019 and in numbers in terms of as I

said it's a complex world since we are the full spectrum from driving assist until you know Robo taxi consumer

autonomous vehicles so here is kind of a breakdown of our business pillars before

I get into technology so there is a driving assist which is a and you know a

very fascinating and exciting growing field and there we are a tier 2 supplier

we sell a chip with our content algorithms and software to tier ones and

tier ones integrate them into a module salt to the car manufacture then we have

our crowdsource technology for building maps this is was designed for powering

autonomous cars but it has over over the past two years we have found new and and

and quite very interesting new business models for for for these maps so it's

it's a branch on its own so it's a data branch we have an update another data branch

for smart cities I'll say a few few words about it this is again a result a derivative of our crowd-sourced data

technology then we have mobility as a service so mobility as a service is

exactly what it means it means that we're not just applying the chip or

supplying a full I a full self-driving system hardware software sensors that we

will purchase in the future we have an activity of building also where sensors

so we're talking about a full stack from end to end give me a car and I'll make

it autonomous it's a full stack but mobility as a service is more than that mobility as a service is older that the

service layers of becoming a service provider it's a fleet optimization fleet a route

optimization fleet management mobility intelligence customer-facing apps teleoperation

back-office all of that we are gradually building its building it is partnering in the future we're open to in organic

growth as was stated by by Intel in the past so it's it's a full it's a full and

very very deep entry and to mobility as a service so to say it in words that we

all know it's becoming an uber lift in the area of Robo taxi then comes the

last pillar which is the full stacks operating system but for consumer autonomous se vehicles that we believe

will come a step after a Robo taxi but we are designing things today that by 2025 we'll be able to price a

self-driving system end-to-end sensors Hardware cables everything below five

thousand dollars that price point enables installing these systems as

built-in into consumer vehicles again at the beginning premium vehicles but this

is where things start starts with premium and then and then trickles it down so as you see it's kind of a

complex world from the business side from technology side it's even more complex so this is in more details the

kind of the offering in driving assist it's level-1 level-2 driving assist we

are the Tier two there's a new category called in driving assist called level two way plus we talked about it last year I'll

say a bit more bit more details today then there is the level four level five

mobility as a service here we're building the self-driving system stack completely and and to end to power our

own Robo taxi but we can sell the self-driving system to Robo taxi makers

as well and then the holy grail of this entire business is what you as a

consumer can go and buy a passenger car which is autonomously enabled so

whenever you want you sit in the back seat press a button and we'll take you to wherever you want to go this is this is the holy grail and this is what we're

planning for there we go back to becoming a supplier to a car - a car

maker and then there is a data data opportunity which is becoming a business on its own so let's say a few

words about the driving assist segment and what I want to point out is this new

category called level 2 plus so level 2 plus means a lot of things it's a term

that many suppliers are starting to use so I started here let's let's define it

in some way so we define level 2 plus as when you have either a multi-camera

sensing so more than just a front facing camera it could be multiple front facing cameras it could be facing camera plus

the parking cameras it could be this plus surround the cameras the full surround a setup of sensing and using HD

maps as part of a driving assist when I talk about HD maps I really refer to our

ramp technology the crowd-sourced technology for building maps automatically using ADA's enabled

vehicles in terms of other functionality so this is the attribute the hardware in

terms of functionality we're talking about everywhere and old speed Lane centering so you can also in a city environment in

a metropolitan environment also have Lane centering and also conditional hands-free driving or conditional

automation in which the driver is responsible for a period of time could be seconds could be minutes could be

there a driving monitoring system that watches the driver and in certain

conditions enables the driver to let go of the driving experience so this is called conditional automation and you

would like this conditional to mention to be everywhere at all speeds and and everywhere this is this is the promise

of level 2 plus and again we're talking about the price point which fits a AAA

volume production so we're not talking about many many thousands of dollars in pork arm this is why the camera system

is so is so critical first it enables this type of functionality and second

the price point of cameras is really nothing no it's it's the lowest cost

sensor that you can that you can imagine so you have cameras plus compute and that's it and you see here a chart from

research in terms of the the growth of this category level to level two plus so

it's becoming a significant category and you can see also from a business I'll

switch the next slides from a business standpoint we have more than seventy

percent you know capacity of level two where systems running today and here are

a number of car models that have a level two plus like the Nissan Pro pilot was

which has a trifocal sensor at the front that uses also HD Maps the false wagon

travel assist uses our HD maps and and so forth and so forth and in and on top of that there are twelve active programs

and thirteen open rfqs in this field so it's a growing field and it's it's an

important field because from a business standpoint we have here the possibility of a SP that grows from a three factor

to fifteen factor of the normal level one level two so it's both exciting from

a value proposition but also from a business proposition and then what comes

after that still in driving assist is what we call vision zero so we we adopted a known term we simply redefined

it so we call this a vision zero in which the car would would do preventive

measures not emergency measures like today's a B systems or preventive

measures too if you have a full surround the system we can use our RSS technology

we published a paper last year showing that we can adopt this technology to human driven policies driving their

policies and used the the RSS guarantees to prove to guarantee that if all cars

have the system there'll be no accidents so I think this is this is the the more

horizon there is nothing yet in this in this category but I we are working

diligently on making this a new category say five to ten years five to ten years

from now so driving assist has the potential of moving from a front sensing camera to a full surround

and singing sweet so it's a huge value proposition but also a huge business

potential let's look so what is powering the

opportunity that I just meant and also autonomous driving that I'm going to show is surround computer vision so I'd

like to get into a bit more details about what are we doing in the surround computer vision so so so surround

computer vision we have cameras around the car 360 degrees and we're processing

the information from those cameras to enable functional value like level 2

plus and I'll say white is also relevant for autonomous driving so at this slide

is a bit of a mouthful but it is important a slide so what is our goal

first goal is that we would like to enable just with cameras a full-stack autonomous driving full-stack from

beginning to end do everything that is necessary to do to support autonomous driving without relying on any other

sensor no radar no no letters now we are targeting a mean time between failure of

one every 10,000 hours of driving which is just to give you an idea if we are

driving two hours a day it will take 10 years to travel 10,000 hours so this is a significant number and I'd like to

motivate why are we targeting this number and how we're building the technology to support this this number

so here's is the why when you when you look at human statistics 10 to the power

of minus 4 or 10,000 hours of driving is the probability of an accident with

injuries 10 to the power of 6 1 million hours of driving or around that a number

is a probability of an accident with fatalities taking safety margins on top

of that we look at 10 to the power of 7 so we would like a critical error error

1 every 10 million hours of of driving which is a credible number there's no

technology today that can reach those those numbers so scratching our head

a few years ago so how do we reach this number and the way to reaches number is to create redundancies but to redundancy

to create not a fusion system which is the dominant school of thought and in

the industry but actually to separate to have separate streams one stream is only

cameras and other stream is only radars and each one of them to reach a ten to the power of minus 4 and then because

those systems are approximately independent a product of them will give

us a 10 to the power of minus 8 and with safety margins and because they are not really statistically independent we'll

reach our 10 to the minus 7 so what we have here so now let's go back to the

computer vision stream so at mobile I we have to two separate vehicles and when

we have customers coming to visit us we show them a demonstration separately of those two vehicles one vehicle which I'm

going to spend more time today is only cameras the other vehicle is only letters and and riders and both vehicles

they do a complete end-to-end autonomous driving so let's go back to the cameras

so we need to prove to ourselves first that we can reach this incredible number of having a computer vision system

making a critical mistake once every 10,000 hours of driving and I just gave

here an example if we drive two hours a day this is equivalent of 10 years of driving so making a critical mistake

once every 10 years of driving and then the question is how do you do that right

so it's not it's not enough to show a demo that we can drive using cameras we

need to convince ourselves that we are building the technology in a way that can support this incredible number ok

because we always tell ourselves you know we're not in the business of science projects it's not an academic

exercise we want to build a business and we want to build a business that can survive the god forbid accidents that

are going to be out there right so we need to convince ourselves that we're doing something very very responsible before we put these machines and on the

road so when you look at our sensing system who are the customers ok so now

we are talking there's no more mention of radars no more mention of whatever I say is only cameras okay so

we have three customers for this camera sensing system one of them is our

driving policy so we're building a car that drives autonomously the car needs to make decisions the decision process

is called driving policy policy is a word from control theory from

reinforcement learning and since we are doing a policy in the area of driving we call this driving policy so the driving

policy is the customer to the output of the camera system because you can think

of all sorts of outputs that you would like from a camera system but that'll be

just you know groping in the in the air right you need to really have a real customer in order to fine-tune what you

want out of a camera system and the customer is a driving policy if the

output from the camera system is actionable from from the point of your driving possible and a driving policy and you can do end to end of the normal

driving then you define correctly what is the output of the camera system so this is one customer the second customer

is our REM technology the harvesting to build maps eight HD maps automatically

so what the camera does in an ADIS environment it captures information from the scene packages the map in a in a

very small bandwidth of 10 kilobytes per kilometer sends them to the cloud and in

the cloud we we have technology to build high-definition maps and automatically and I'm going to have a section in this

today a bit about that the third customer for our sensing

system is our driving assistive products

the level-1 level-2 and what i mentioned at the beginning is the level 2 plus this is also a customer for our sensing

gate system so when we break down the computer vision in this fully autonomous

a world in which driving assist becomes a special special case we have the detection we divide them into four

buckets one of them are about road users detecting all vehicles and other road

users were cyclists and pedestrians and and people on scooters and whatever road

users these are dynamic and this area is very very rich in terms

of how you specify it and I'll show some more details about it then there is a road geometry everything about the

draggable paths that that we see so it's not only the lane marks we need to we need to break down the scene into

drivable paths and understand well what each drivable path means then there's Road boundaries all the delimiters could

be a kerb could be a guardrail all the delimiters that we found that we find in

on the road and then the road semantics what can we infer from a semantic point

of view from from the information that we see this is traffic signs traffic

lights a pavement markings and a net and so forth so these are the four buckets

now now comes the task of reaching the ten ten to the power of minus four this

is an incredible number so the way we approach this is we create internal redundancies these redundancies are not

statistically independent but they take for example vehicle detection rather

than building one whoremongers black box network deep technology for detecting

vehicles we do it multiple times using different algorithms these are these internal redundancies and each one of

those algorithms end-to-end so doesn't rely on other on other algorithms so and in this way again we

don't need to prove ourself statistical independence it's just a a software design approach to do the same tasks

using different approaches but each each approach doesn't rely on on on the rest

so the way we look at it you know there are several sources of information one is appearance now you have pixels in the

image this is appearance there a vehicle has an appearance a typical appearance

and you use that typical appearance in all the training network to to Detective vehicles for example then there is a

geometry things that we find from perspective from a motion all sorts of

classical things that that we as humans use in order to extract 3d even though

our sensors to dimension our two dimensional sensor camera the two dimensional sensor we see 3d implicitly

we extract through the information we infer 3d information we don't have direct access to 3d information this is

called geometry based so the same thing for detection and also for for measurement measurement meaning lifting

the two dimensional information to to 3d because in order to control the vehicle everything needs to be situated in a 3d

environment so we need to lift all the two-dimensional detections into 3d and here also we have appearance and we have

geometry and again the idea is to use them as separate tracks not combining

all this information into one algorithm but using separate tracks so let me give you an example let's look at I'm going

to focus now on the road users so let's look at you know there is a bus a white

bus here and in the scene and we're detecting a road user using six different algorithms one of them is the

classical pattern recognition you are putting your detecting a three-dimensional box situated on on a

vehicle using pattern recognition so you have your train and network that scans

the the image and tries to to find an area which you can put a box around it that will fit the appearance of of a

vehicle this is the classical when people talk about bedico detection they will normally refer to this there is

another one which is called full image detection say you have a large truck or

large bus very very close to you few centimeters away from you this is not something that the easily putting a

bounding box on it you need to approach it it differently it's still appearance but it's a completely different

algorithm on how to handle very very close objects and I'll show you some examples later then we have a free space

we call this top view free space so the green all these pixels of green is where the road is and whatever whatever is not

road its suspect to being an object so this is another way to to find and to do

object detection then we have features like wheels we use them also separately to define objects then we have what is

called in the academic environmental so it's ranking ladder but

with cameras so the fact that we have multiple cameras we can use it in order to generate a three-dimensional percept

of the world using triangulation classic triangulation but we have multiple cameras and in the car and then we can

feed that data into the ladder processing algorithms as we said we have two separate streams so we have already

algorithms that take ladder information and create an environmental model instead of feeding at ladder we feed it

the information coming from the stereoscopic multi camera stereo so that

this is completely different track both for detecting objects and and measuring information so rather than you rather

than developing an algorithm that takes all these sources of information and outputs the objects we have six

different algorithms in order to create those internal redundancies the same thing for for measurement I want to lift

from two-dimensional to 3d the visor of course gives me 3d information the

visual road model using classical computer vision of a plane plus a

parallax perspective optic flow we can get a road model and use that in order

to get depth we have a map in HD map which we build using our REM technology that also gives us information about the

shape of the road which we use to extract the depth and we have another network that uses appearance in order to

estimate range and so each one of them is a separate track rather than combining them all altogether we have

different engines to create the lift from 2d to 3d and again but what's

underlying all of that is reaching this ten to the minus four probability right because we it's not enough just to show

we can do autonomous driving we have to do it in a way that at the end when we

put all those dreams together we can get a very very low probability of mistake which is at least as good as humans but

much better than humans has to be much better than humans okay so now let's go examples of each so this is the full

image detection if if you look here when the object is very very close to you it

becomes a very big challenge on detecting it as an object because there's really nothing here to say that there's a

there's an object let's look at this as well now this is important when you are

maneuvering in autonomous driving situation when you're maneuvering in very narrow situations it's a a two-way

road there's oncoming traffic there's blockages and now you are driving and the other road users are

centimeters away from you so it's a completely different approach on how to detect how to detect an object let me

show you an example of how this looks like so what you see here this is the top view from the system we're getting

closer to a blockage and now you can see this vehicle coming in here and very

very close these are the kind of situation in which you need this full image detection and passing through and

you can see this it's really autonomous this is a drone view I'll chop plenty of

more examples so this is one of them another is how do you track how do you

track objects among gay cameras so this is a network that uses what is called a

signature this is a technique from face recognition where you take an image of a face and you map it into a vector of

numbers which is called a signature and that vector of numbers is invariant to facial expressions whether you change

your hairstyle and and so forth so the same thing is being done here where you use a signature of an object and find

that signature in other in other camera so you can track very very long baseline tracking of objects this is a range net

so using appearance in order to estimate range so what you see here this is the classical way of measuring range using

some a Kalman filtering so it stabilizes over over time this is number of frames

and the the red bar is the true range and you see here we're using a range net

using the appearance to estimate depth we can get immediately the range from a

single from a single day frame so this doesn't come instead of the classical this is on top of the of the

then another another engine is pixel level segmentation so we can see here

that every pixel is segmented the color of the pixel is what it is so green it is Road and you see here the limiters

like Road edge are are in in red let me run this again and what you see below is

a segmentation into objects so each color is a separate separate object here

we're coloring only the road users so the cars and and the pedestrian let me run this again so that you can see so

this is the pixel segmentation so every type of object has the same color and here this is the object detection so

every color is an is an object and this is the same thing at night so again so

this is a stream running parallel to everything else in the system this is not an engine that is that is combined

with other engines of the system this is a completely separate subsystem this is

from this around a camera so same thing being computed from every from every camera then we tell ourselves there are

certain objects that occur frequently those objects that occur frequently would like to have a model based

detection so to train a network for that particular class of home to study doors a door is tricky because it's hovering

above the ground plane so to get measurement to something hovering about above the ground plane is tricky we need

to come with wouldn't talk about cameras and it sufficiently important from a safety critical point of view that you

want to do something dedicated for it so you can see that doors are detected as a

separate class of objects and on the right hand side you can see how it is in

the you see the grid point here this is the door and how the autonomous car is

is maneuvering its way around the open door calculating a new trajectory in

order to bypass the open the open door another separate class of object are

strollers you can see here the the stroller sufficiently important from a safety critical point of view last thing

that you want to do is to hit a child so it warrants a separate detector and

you can see on the right hand side an example of a stroller coming in and the

autonomous car is taking notice of it then comes another engine this is the

parallax net so this is using geometry so every pixel is now colored based on the height from the ground plane so red

is above the the height plane blue is below to get the ground plane and the ground plane is not necessarily a a

plane it could be something that is that is curved and this is a network that is trained also on moving object so it's

not only stationary stationary a world so again this is used this is a separate engine that is used for measurement and

used also for the technical jek tours if something is above the ground plane at least you know there's something there before you even know if it's a car or

not a car then there is what we call it

we brilliant coined this name that's in the academic circles it's called a radar so it's a deep network that takes the

multiple cameras that we have in the car and combines them all together to do triangulation and get a dense 3d percept

there are many uses for it one of them there are all sorts of hovering objects protruding objects that you need to know

about and so you need you need precise 3d so these are examples of the cameras

that their images are fit into this kind of network and then when you look at it

you get a 3d perceptron a ladder but much higher resolution because we're talking about the cameras from the

existing cameras in the car so it's not that we have stereo units we have 12

cameras around around the car so say it's a long baseline stereo in in the

Kadena case circle here's another clip while we're moving so so again so this

is now a separate route a separate engine it's not combined with all other other engines that I mentioned before it

separate engine that can be used for detecting object and also measuring a range of those to those objects and

here's an example how we use the VAD our data and feed it into the ladder processing stream that we have in order

to detect object so we take the same algorithms that we have with the vehicle with radars and ladders and feed it with

the vendor that invite our information rather than with the ladder information in order to get a stream word with

detect objects and and measure measure range to them here we can see how these

different type of streams of information help us to avoid to define obstacles so

here is a route in which there are many of cars that are obstacles and the system needs to figure out whether it's

an obstacle that you need to overtake or whether it's a traffic jam that you need to simply wait and wait that it is

cleared and it's kind of tricky because your your view is limited and the object

which is the obstruction is occluding but there are all sorts of tails like if

you do a pixel labeling and you know the pixels of the road you also you see pixels of road behind the obstruction so

you know it's not a traffic jam so it's putting all this information together in order to understand what our obstacles

and and and overtake them then there's a semantics knowing about a blinkers

emergency vehicles knowing what pedestrians are doing let me run this

again because at the beginning we see what this is okay so we we know the

orientation of the pedestrian we can go deeper than that and also understand

hand gestures so to know what it means come closer you can pass stop I'm on the

phone all this is also part of the system as part of semantics so now let

me put all of this together and I'm going to show you a clip from our autonomous car again so we're talking

about this stream with only cameras so it's a car fit with 12 cameras it's 8 long-range cameras

and for parking cameras in terms of the cost it's really nothing hundreds of

dollars and it's all running one iq5 chip the hardware has two IQ five chips

but right now what is being processed only one IQ five chip so it's one IQ 5 chip just to give an idea what this

means everybody talks about chips and so forth so an IQ 5 chip has two dies each die is a 7 nanometer process it's 45

millimeter square area so we're talking about a 90 millimeter square area running what I'm going to show you so 19

millimeter square for a chip nothing you know all these humongous chips that you

know Zeon's and other these are hundreds of millimeter square chips hundreds plus or even two to one thousand so we're

talking about very very small footprint very very lean compute going to do what I'm going to show you right now so this

vehicle 12 cameras one IQ five chip taking all the information from the cameras and doing an end-to-end

autonomous driving now we are developing this with the mindset of being able to

be agile as a human so it's not only from an algorithmic point of view there's also the our model of safety

which is called RSS I'll say a few words about this later putting those together enables a agile

driving now agile driving is very very important because the first use case of autonomous driving is going to be Robo

taxi before we can go and purchase a car it's going to be Robo taxis for a number

of years now Robo taxi is a business as a consumer you go in and hail a Robo

taxi and it should take you from point A to point B who is going to hail a Robo taxi knowing that's going to take twice

as time to reach its destination so it has to be agile it has to drive like like a human so this is in the center of

Jerusalem the driving style of Jerusalem is Boston Plus outside so it's really

challenging people are not patient so if you want to merge into traffic you need to show your intent and you need to be

serious about your your intent just turning the turn see the turn signal wouldn't do it right it's up okay so now

so let's have a look at this so we have here three panes this is from a top view this is a drone this is the autonomous

car you see here the Intel mobile I logo this is what the computer sees kind of

360 and this is just to show you that we're not cheating it's really driving

okay this is sped twice as fast because

in a presentation I want to finish this fast so here's the autonomous car so you

can see that now it's a very narrow corridor oncoming traffic and it needs

to find its way so let it do it it's a one-minute clip so it will be will be

over quickly and now it's going to enter into a unprotected left turn and now see

what's what's going to happen okay now we're showing you the pixel labeling so

this huge bus simply stops okay so now

now there is occlusion so what do you do I need to do an unprotected left so now it's pushing its way now there's a

pedestrian going pedesta coming you see this pedestrian alright nothing it's way

doesn't Jerusalem okay and has to push

its way through okay

so we're talking about a system with a bill of material about one thousand dollars that's what you see that the

proportions about it and and this is one stream that's another stream of radars

and ladders that at the end of the development will be put together to reach ten point - a seven probability

okay I'll skip this so in order to show that we're not simply taking a section a

few seconds out of ten thousand unsuccessful second affections and

showing you only the successful section we put this morning then tire out it's a

20-minute ride starting from a Bly ending at a mobile I into the heart of

heart of Jerusalem it's available it's in in 4k so and it's in original speed

so it's not fast forwarded so you can see everything you can see the strength you can see the weaknesses I don't think

that there are weaknesses but if they are you can see them and for all to see

alright so it's I think it's very very powerful I've seen this morning a discussion and reddit pages and pages

and pages of discussion I think this is what the industry needs to be transparent to show that noble what

we're doing and not to be shy about it so it's all the entire route is

available at the YouTube and our YouTube channel ok so now let's go into a

mapping which is a very important pillar it's an important pillar to power at

enormous driving but it has other other business use cases so I mentioned before its ADA's enabled vehicles which are

tens of millions of those those vehicles so to power them in a way in which

critical information from the scene can be extracted sent to the cloud in the cloud we call this aggregation we built

high Denish a television a maps and then we send them back to cars in the level two plus category and of course our

autonomous driving critically relies on on these HD these HD maps so in terms of

harvesting the three car makers that we announced two years ago

BMW needs Volkswagon already has been launched we receive six million kilometers per day

of data and we'll be able to map all of Europe by next quarter and the entire US

by the end by the end of the year and we have additional three OEMs that I am not

at liberty to say their names but we have six OMS that we have contracts for sending data by this year would have 1

million a car sending data by 2022 it's supposed to be about 14 million cars

sending some of the data so that this is very very powerful you can see here this is data from the

BMW vehicles so that BMW was the first to launch so you see the coverage after

a few weeks coverage in the US so this tells you where are the areas that people can afford buying a BMW ok this

is this is a Europe and as I said all of

Europe by next quarter most of the US by to the end of this end of this year and

ok so what are we doing with this data today our technology we can we can

automatically so again no manual intervention the data coming from coming from the cars going to the cloud we

automatically build high definition BAPS for all roads about 45 miles per hour so

I'm not talking about highways it's more than highways 44 45 miles per hour is El

Camino in in Palo Alto where you drive our traffic lights traffic light of the traffic light of the traffic light

that's 45 miles per hour so we're talking about the rich set of roads that today are mapped completely

automatically of course our ambitious is more than that we believe that full

automation of all types of roads by 2021 okay here's an example of kind of the of

the mapping this is this is in Las Vegas so it sits here and so all this is done automatically

in terms of China China's is a very important territory for us and it's

growing in all aspects and all the spectrum of our activity in ada's 25% of

all our design wins in 2019 or in the Chinese or in the Chinese market so in

terms of RAM in China right again we need to comply with the regulations in

in China which are very complex so in terms of we have a joint venture that

was signed a couple of months ago with the Cinque University uni group with a

mapmaker as part of the joint venture to allow us to start collecting data for

for maps we announced a month ago with the Neo they'll be using their fleet of

tens of thousands of vehicles to start harvesting data with neo it's more than

that they'll be taking our self-driving system and using it as built-in into their car starting for 2022 that'll be

our first kind of trial and consumer AV in in China and then recently this is

news it was announced this morning that with the psyche together with their map

maker called heading to start harvesting data and using the High Definition maps to power

level 2 plus with the in China by Shanghai automotive so we can see that

we are building the right the right steps in order to be very active in

China now there's also smart cities opportunities so the data that we collect from vehicles can power smart

cities and the more we we study this market the bigger it looks so there's

infrastructure asset inventory we have a strategical operation with the UK mapping a company we announced a year

ago there is pavement condition assessment that municipalities pay every year to do

the survey there's dynamic mobility what happens with good road users with

pedestrian straight walkers and so forth that is data that municipality is pay

for the market is billions of the really we study it very very detailed

and and here's an example of harvesting which we collect everything that is

useful in terms of survey asset inventory from from the scene and send

it to to the cloud here's an example of a pavement a

condition so this is an area in in Israel in which we color code the

pavement condition so red is poor green is fine here is an example of a poor

pavement condition the next picture shows you how bad the road is here's

another example where you have a pothole also poor pavement condition but by the way detecting potholes and cracks that

could become potholes is very very important because when when consumers

damage their vehicle on a pothole the city is liable and it's millions of

dollars all right so if you can know in advance that you have a crack and you go and handle it then it saves a lot of money to do a

municipality I mean now all of this is done automatically there is really zero

cost to do all of this it's an ADA's camera a car with a Nader's enabled camera this country for information is

very low bandwidth sent to the cloud and we can then create value to to the

Commonwealth of cities here's another area of road condition score this is you

see here the crack again all of this is automatic okay let's go into the next

pillar which is the the RSS so one of

the one of the ingredients a critical ingredient in enabling then very very nice demonstration that you have seen

before in Jerusalem it's not only the power of the algorithmic approach how

you do decision-making but it's also the way it is the model of safety because

you need to define what it means to be safe now it sounds simple but it is not

when you look at traffic laws there are many things that are well defined now we reach a red traffic light we need to

stop when you reach a yield sign we need to well done when he reaches stop sign we need to stop these other well define

things but then there is this duty of care kind of bucket which is you need to

be careful now what does it mean to be careful that our societal norms being careful in Boston is different from

being careful in Phoenix in Arizona so what does it mean to be to be careful there societal norms about that there

are legal presidencies so humans can live with ambiguity machines cannot so

we need to define what it means to be worth it to be in a dangerous situation because if you don't have a formal

definition then you become conservative because you don't take risks because you

don't you haven't defined what dangerous is and this is what RSS is is doing it

is defining a in in layman terms the idea is to define assumptions that

we humans make about other road users so for example if I'm driving in a multiple in multi-lane highway and there's a car

next to me I'm assuming that that car is not going to spontaneously turn the the wheel the steering wheel and hit me

because if this is if this is something that could happen I'll never drive side by side to another vehicle right so we

make assumptions if we are driving behind another car we keep a safe distance we are making assumption about

what is the braking force again this is very very implicit we are not mathematicians right but you know we are

making assumptions what is the reasonable braking force and based on that we maintain a safe distance when we

are reaching when we have the right-of-way and there is another vehicle in a in a secondary road that needs to the needs to stop we know that

right-of-way is given not taken so we kind of calculate to ourselves is this car does this car can stop to give me

the right-of-way if it cannot stop it's coming too fast it cannot stop I'll not take my right-of-way I'll give way right

so we make assumptions so the idea is to make those assumptions form or make

those assumptions clear so that we can engage with regulatory bodies about these assumptions some of these

assumptions are parameters we want to set these parameters based on data and based on engagement with regulatory

bodies and then the model takes the worst case means that we're not

predicting what the other driver would do we're taking the worst case once we made clear what our assumptions are then we are

taking the worst case and then we need a mathematical model to show that we can apply the inductive principle such that

we we're never in we never get into a dangerous situation as defined by the

assumptions and if we are in a dangerous situation because of others that put us into a dangerous situation what the

proper response to get out of it without causing anyone else to be in a dangerous situation and then theory that proves

that if all actors behave this way it'll never be accident that that's what RSS is and I said everything is to say about

it in terms of so this paper was published two years ago and we have been working diligently it was this area was

managed by led by by by Intel to start having a discussion with regulatory

bodies with industry and players with partners with car makers to start

standardizing it and again the technology is built in a way that is technology neutral it doesn't have any

benefit to Mobileye or to any particular actor it is the understanding that if we don't standardize it no one will be able

to launch autonomous driving maybe for demonstrations but not as but not as a

business so the last day which was announced a few weeks ago there's a new

I Triple E body which is established it based it's inspired by the RSS principle

the chair is chaired by by by Intel by Jack waste from from Intel and there has

been also safety report mentioning RSS in their definition also endorsement by

by all sorts of actors in China it's even gone further than that there was a

technical body that was established a month after the RSS was was published and has finished its work

it basically standardized RSS in China it will take effect March 2020

so the Chinese are working fast and we believe it's a long process it's a long

journey but eventually this needs to be standard wherever turn almost driving is going to be going

to be deployed okay and last thing is

our mobility as a service the business status again remember we had level two

plus we're building autonomous driving we're building a self-driving system we're also building the entire stack for

a service mobility and as a service we mentioned a few months ago was last year

that we have a joint venture with false wagon and champion motors in Israel to

launch a commercial mobility as a service early 2002 22 so no driver

behind the steering wheel it'll be about 200 vehicles in Tel Aviv and then it will scale to all of Israel

and then of course we'll use all the lessons learned and and to to ramp out

outside of Israel in parallel we have established this was announced a few

weeks ago our ATP is is the biggest public operator in France and we have we

have an agreement to work together on on deploying commercially self-driving in

Paris it will start October this year with the driver or the safety driver taking the kind of car that you have

seen that I've shown it's going to be a number of them in Paris taking passengers in October 2020 and this year

where the work is towards removing the driver 2020 to 2023 timeframe we

announced today we have something very similar to this joint venture in the

Indigo a city in in South Korea and deployment commercial deployment 2022 so

while we're deploying in Tel Aviv will deploy in France will deploy in South Korea we mentioned a month ago the deal

with the neo there es8 vehicle will be

used as our Robo taxi neo will be taking our self-driving system and in 20 to 20

22 timeframe they'll launch in China vehicles of consumer AV and those

vehicles we can purchase them and use them from boo taxi fleet as well so it's a double

advantage one it's our first experiment with consumer and a thing in China and also we have a platform for our Robo

taxi in terms of hardware the hardware that is running today has to 2 IQ fives

what will be launched in 2022 is the hardware with 6 IQ fives I don't think

that we'll need 6 but we don't want to take risks so to have enough computing

power to do whatever we wish to do and this is what this is very very lean in terms of compute this is going to be

ruggedized automotive qualified IQ 6 is

coming out is going to be sampled and in this year and will deploy it in 2023 so

the entire hardware would be running on a single IQ 6 chip and this is our path

towards 2025 to reduce the cost of a self-driving system to below $5,000 our

self-driving system here for the Tel Aviv for the joint ventures that I mentioned before Tel Aviv France South

Korea will be around somewhere between 10 to 15 thousand dollars the cost will

be able to downsize it to less than five thousand in 2025 and this is critical

five thousand becomes a reasonable number for consumer AV and again premium cars but this is how things starts it

starts with their premium cars so to summarize the main takeaways the first

the level 2 plus is a growing new category both in terms of incredible

value to a consumers and also a very very interesting business a potential

okay and surround computer vision unlocks considerable value realization

of a safe level for and also unlocking the full potential level 2 plus requires surround computer vision and here we

need to build it as a standalone and to end quality as I showed you in the clip just with cameras we can do self-driving

it's not going to be safe enough we need radars and ladders as a separate stream but in a level 2 plus context in which a

driver is responsible this this is very very critical the fact that we can we can support a full end-to-end autonomous

driving just with computer vision and that requires considerable thought this is these are all these internal

redundancies that that I mentioned not enough to show that we can detect an object we need to show that we can

detect an object at infinitesimal probabilities of mistakes and for that

we need all those internal redundancies level 2 plus requires HD maps but those

HD maps are not confined to a zip code they have to be everywhere this is where the power of our crowd-sourced

technology comes in consumer AV requires

HD maps everywhere again this is where the power of our crowd source technology comes comes to play the crowd-sourced

the RAM data is a great value to smart cities this is a new business new business line that we are building RSS

regulat we call this regulatory science this needs to spread more and more to be

evangelized to be transparent in order to engage with regulatory bodies in order to take autonomous driving from

the realm of a science project to real Moffit of a real business and then the

road to consumer AV goes through Robo taxi there is no way to jump from where we are today to consumer AV without

going through the Robo taxi phase there are a number of reasons one of them is cost that I mention another is from a

regulatory standpoint much easier to regulate an operator than to regulate the consumer an operator operates a

fleet of vehicles has reporting responsibilities as back off its responsibilities tailor operations

responsibilities all of that you cannot put on a consumer so you need a few years of Robo taxi before you go from a

regulatory standpoint before you go to consumer and anything this is why Robo

taxi is our next step and all these ventures that we mentioned with the false wagon in Tel Aviv with the

with France is in the Robo taxi area we build a robot taxi in as a step to go to

consumer AV and while we're doing that we build a business which is the ability as the surface thank you